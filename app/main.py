import os
import streamlit as st
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import ChatMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
from langchain_community.vectorstores.faiss import FAISS
from langserve import RemoteRunnable
from langchain_openai import ChatOpenAI
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler


# â­ï¸ Embedding ì„¤ì •
# USE_BGE_EMBEDDING = True ë¡œ ì„¤ì •ì‹œ HuggingFace BAAI/bge-m3 ì„ë² ë”© ì‚¬ìš© (2.7GB ë‹¤ìš´ë¡œë“œ ì‹œê°„ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)
# USE_BGE_EMBEDDING = False ë¡œ ì„¤ì •ì‹œ OpenAIEmbeddings ì‚¬ìš© (OPENAI_API_KEY ì…ë ¥ í•„ìš”. ê³¼ê¸ˆ)
USE_BGE_EMBEDDING = True

#if not USE_BGE_EMBEDDING:
    # OPENAI API KEY ì…ë ¥
    # Embedding ì„ ë¬´ë£Œ í•œê¸€ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•˜ë©´ í•„ìš” ì—†ìŒ!
    # os.environ["OPENAI_API_KEY"] = ""

# â­ï¸ LangServe ëª¨ë¸ ì„¤ì •(EndPoint)
# 1) REMOTE ì ‘ì†: ë³¸ì¸ì˜ REMOTE LANGSERVE ì£¼ì†Œ ì…ë ¥
# (ì˜ˆì‹œ)
# LANGSERVE_ENDPOINT = "https://poodle-deep-marmot.ngrok-free.app/llm/"
LANGSERVE_ENDPOINT = "http://localhost:8000/chat/"

# ë¡œì»¬ ë””ë ‰í† ë¦¬ ì„¤ì •
LOCAL_DOCS_DIR = "C:\\Users\\slek9\\Desktop\\aa"

# 2) LocalHost ì ‘ì†: ëì— ë¶™ëŠ” N4XyA ëŠ” ê°ì ë‹¤ë¥´ë‹ˆ
# http://localhost:8000/llm/playground ì—ì„œ python SDK ì—ì„œ í™•ì¸!
# LANGSERVE_ENDPOINT = "http://localhost:8000/llm/c/N4XyA"

# í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„± @Mineru
if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

# # í”„ë¡¬í”„íŠ¸ë¥¼ ììœ ë¡­ê²Œ ìˆ˜ì •í•´ ë³´ì„¸ìš”!
# RAG_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì „ë¬¸ ë³€í˜¸ì‚¬ ì…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥ì„ ì´í•´í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.
# Question: {question}
# Context: {context}
# Answer:"""


RAG_PROMPT_TEMPLATE = """You are a competent artificial intelligence legal assistant. Answer questions in the following format. Answer in your language and say you don't know if you don't know.

Question: {question}

Context: {context}

Case Summary:
[Summarize the key points of the case based on the question and context]

Relevant Statutory Provisions:
[statutory_provisions]

Analysis:
[Provide an objective analysis of the case based on the given information and relevant statutes]

Conclusion:
[Summarize the key points of the analysis]

IMPORTANT: I am a legal assistant AI, not a legal professional. This information is for informational purposes only. You should always consult with a qualified legal professional before proceeding with any legal matter or making any legal decisions.
"""

st.set_page_config(page_title="LawGPT", page_icon="ğŸ›ï¸")
st.title("âš–ï¸ LawGPT")


if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"),
    ]

with st.sidebar:
    st.link_button("ğŸğŸ™ í›„ì›í•˜ê¸°", "https://toss.me/lawgpt")

def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)


def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))


def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)


@st.cache_resource(show_spinner="Embedding file...")
def embed_files_from_directory(directory):
    """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ë“¤ì„ ì½ì–´ì™€ ì„ë² ë”©í•©ë‹ˆë‹¤."""
    retriever = None
    for filename in os.listdir(directory):
        filepath = os.path.join(directory, filename)
        if os.path.isfile(filepath):
            file_extension = os.path.splitext(filepath)[1].lower()
            if file_extension in [".pdf", ".txt", ".docx"]:
                retriever = embed_file(filepath)
    return retriever

@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file_path):
    """ë‹¨ì¼ íŒŒì¼ì„ ì„ë² ë”©í•©ë‹ˆë‹¤."""
    cache_dir = LocalFileStore(
        f"./.cache/embeddings/{os.path.basename(file_path)}"
    )

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "(?<=\. )", " ", ""],
        length_function=len,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=text_splitter)

    if USE_BGE_EMBEDDING:
        model_name = "BAAI/bge-m3"
        model_kwargs = {"device": "cuda"}
        encode_kwargs = {"normalize_embeddings": True}
        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs,
        )
    else:
        embeddings = OpenAIEmbeddings()

    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, embedding=cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever





# with st.sidebar:
#     file = st.file_uploader(
#         "íŒŒì¼ ì—…ë¡œë“œ",
#         type=["pdf", "txt", "docx"],
#     )
#
# if file:
#     retriever = embed_file(file)

retriever = embed_files_from_directory(LOCAL_DOCS_DIR)

print_history()


if user_input := st.chat_input(placeholder="ì„ëŒ€ì°¨ ê³„ì•½ ë§Œë£Œ í›„ ë³´ì¦ê¸ˆ ë°˜í™˜ ë¬¸ì œ ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?"):
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        # ngrok remote ì£¼ì†Œ ì„¤ì •
        ollama = RemoteRunnable(LANGSERVE_ENDPOINT)
        # LM Studio ëª¨ë¸ ì„¤ì •
        # ollama = ChatOpenAI(
        #     base_url="http://localhost:1234/v1",
        #     api_key="lm-studio",
        #     model="teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf",
        #     streaming=True,
        #     callbacks=[StreamingStdOutCallbackHandler()],  # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì¶”ê°€
        # )
        chat_container = st.empty()
        if retriever is not None:
            prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            rag_chain = (
                {
                    "context": retriever | format_docs,
                    "question": RunnablePassthrough(),
                }
                | prompt
                | ollama
                | StrOutputParser()
            )
            # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜ë¥¼ ì…ë ¥í•˜ê³ , ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
            answer = rag_chain.stream(user_input)  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))

        else:
            prompt = ChatPromptTemplate.from_template(
                "ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”:\n{input}"
            )

            # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
            chain = prompt | ollama | StrOutputParser()

            answer = chain.stream(user_input)  # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜
            chunks = []
            for chunk in answer:
                chunks.append(chunk)
                chat_container.markdown("".join(chunks))
            add_history("ai", "".join(chunks))

